---
layout: home
title: Machine Learning Fundamentals
---

# Welcome to Machine Learning Fundamentals ðŸš€

Hey there! Welcome to my corner of the internet where I break down complex machine learning concepts into digestible, engaging explanations. No PhD required - just curiosity and a willingness to learn!

## What You'll Find Here

I believe that the best way to truly understand machine learning is to build intuition from the ground up. That's why I focus on explaining the fundamental algorithms that power everything from your Netflix recommendations to cutting-edge AI systems.

## Featured Posts

### ðŸ“ˆ [Gradient Descent: The Art of Rolling Downhill]({{ site.baseurl }}{% post_url 2025-06-01-gradient-descent %})
*June 1, 2025*

Ever wondered how machines actually "learn"? It all starts with gradient descent - the workhorse optimization algorithm that powers modern AI. In this comprehensive guide, I'll take you from the intuitive "blindfolded on a mountain" analogy all the way through to advanced optimizers like Adam. 

**What you'll learn:**
- The mathematical foundations explained simply
- Three flavors of gradient descent (Batch, Stochastic, Mini-batch)
- Common pitfalls and research-backed solutions
- Why sometimes you need to go up instead of down!

[Read more â†’]({{ site.baseurl }}{% post_url 2025-06-01-gradient-descent %})

---

### ðŸ“Š [Regression: The Art of Drawing Lines Through Chaos]({{ site.baseurl }}{% post_url 2025-06-05-regression %})
*June 5, 2025*

Regression is where many people's machine learning journey begins - and for good reason! It's intuitive, powerful, and forms the foundation for understanding more complex models. This guide takes you from simple linear regression all the way through regularization techniques.

**What you'll learn:**
- Complete implementations from scratch
- When to use Ridge vs Lasso regularization
- The five critical assumptions (and why they matter)
- Real-world tips for better models

[Read more â†’]({{ site.baseurl }}{% post_url 2025-06-05-regression %})

---

### ðŸŽ¯ [Support Vector Machines: Finding the Perfect Boundary]({{ site.baseurl }}{% post_url 2025-06-10-support-vector-machines %})
*June 10, 2025*

Ever wondered how to draw the BEST possible line between two groups? Support Vector Machines don't just find a boundary - they find the one with maximum confidence. In this guide, we'll explore the elegant mathematics behind SVMs and the kernel trick that makes them so powerful.

**What you'll learn:**
- The maximum margin principle and why it matters
- Building an SVM from scratch
- The kernel trick for non-linear problems
- Practical tips for real-world applications

[Read more â†’]({{ site.baseurl }}{% post_url 2025-06-10-support-vector-machines %})

---

### ðŸŒ³ [Decision Trees: The Art of 20 Questions with Data]({{ site.baseurl }}{% post_url 2025-06-15-decision-trees %})
*June 15, 2025*

Remember playing "20 Questions" as a kid? That's exactly how decision trees work! They learn to ask the right questions about your data to make predictions. This guide takes you from the intuitive concept through implementation, visualization, and even into the forest of ensemble methods.

**What you'll learn:**
- How trees split data using information theory
- Building a decision tree classifier from scratch
- Visualizing and interpreting tree decisions
- Preventing overfitting with pruning techniques
- Why Random Forests are better than single trees

[Read more â†’]({{ site.baseurl }}{% post_url 2025-06-15-decision-trees %})

---

### ðŸŽ² [Naive Bayes: The Power of Simple Assumptions]({{ site.baseurl }}{% post_url 2025-06-20-naive-bayes %})
*June 20, 2025*

What if we could solve complex classification problems by making a "naive" assumption that all features are independent? Turns out, this simple idea leads to blazingly fast classifiers that work surprisingly well, especially for text classification. Join me as we explore the probabilistic foundations and practical applications of Naive Bayes.

**What you'll learn:**
- Bayes' theorem and how to update beliefs with evidence
- Why the "naive" assumption is actually brilliant
- Implementing Gaussian, Multinomial, and Bernoulli variants
- Building a spam classifier from scratch
- When simplicity beats complexity

[Read more â†’]({{ site.baseurl }}{% post_url 2025-06-20-naive-bayes %})

---

### ðŸ§  [Neural Networks: Teaching Machines to Think]({{ site.baseurl }}{% post_url 2025-06-25-neural-networks-introduction %})
*June 25, 2025*

Journey from a single artificial neuron to the deep architectures powering modern AI. This comprehensive guide takes you from zero to implementing your own neural network, explaining how these brain-inspired systems learn to recognize patterns, make decisions, and even create art.

**What you'll learn:**
- How artificial neurons mimic biological ones
- Building neural networks from scratch
- The backpropagation algorithm explained clearly
- Famous architectures: CNNs, RNNs, Transformers, and more
- Practical tips for training deep networks

[Read more â†’]({{ site.baseurl }}{% post_url 2025-06-25-neural-networks-introduction %})

---

## About This Blog

I'm a Computer Engineering graduate student at NYU Tandon, and I've been on both sides of the learning curve - from struggling to understand these concepts to teaching them to others. My goal is to create the resource I wish I had when I was starting out.

Each post is crafted to:
- Build intuition before diving into math
- Include working code you can actually run
- Explain the "why" not just the "how"
- Keep things engaging without dumbing them down

## Let's Connect!

Have questions? Found an error? Want to suggest a topic? Feel free to reach out!

- Email: [nishantsharma@gmail.com]
- GitHub: [github.com/nishant-ai]
- LinkedIn: [linkedin.com/in/nishantsh20]

Happy learning! ðŸŽ“

---

*"The best way to learn is to teach. The second best way is to write about it."*